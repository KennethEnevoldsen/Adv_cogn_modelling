---
title: "rescorla_wagner"
author: "K. Enevoldsen"
date: "2/26/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup}
pacman::p_load(pacman, tidyverse, R2jags, modules, patchwork,
               extraDistr # for extra destributions
               )
set.seed(1994)
agents <- modules::import(module = "../jags_helpfuns/agents", attach = T, doc = T)
modules::reload(agents)
qc <- modules::import("../jags_helpfuns/quick_n_clean_plots", attach = T, doc = T)
modules::reload(qc)
jh <- modules::import("../jags_helpfuns/jags_helpfuns", attach = T, doc = T)
modules::reload(jh)
```

```{r payoff}
# generate payoff matrix for bandit task
  # choice of bandit A - 30% of a payoff 2, otherwise 0
  # Choice of bandit B - 70% of a payoff 1, otherwise 0

bandit_payoff <- function(ntrials = 100, choice = "a", a_prop = 0.3, a_reward = 2, b_prop = 0.7, b_reward = 1){
  if (choice == "a"){
    return(rbinom(ntrials, 1, a_prop)*a_reward)
  } else if (choice == "b"){
    return(rbinom(ntrials, 1, b_prop)*b_reward)
  }
}

payoff <- cbind(bandit_payoff(choice = "a"), bandit_payoff(choice = "b"))
```


```{r jags models rb} 
model <- "model {
  theta ~ dunif(0, 1)
  b[1] <- theta
  b[2] <- 1-theta
  
  for (t in 1:n_trials){
    choice[t] ~ dcat(b) # categorical distribution
  }
}
"

writeLines(model, "RB_jags.txt")
```


# sim an fit RB
```{r sim}
rb_sim <- agents$RB(theta = 0.7, payoff = payoff)
rb_sim


# fit
samples <- jags.parallel(data = list(choice = rb_sim$choices, n_trials = length(rb_sim$choices)), 
                inits = NULL, 
                parameters.to.save = c("theta"), 
                model.file =  "RB_jags.txt", 
                n.chains = 4, n.iter = 5000, n.burnin = 1000 # warm-up
                )



qc$plot_dens(x = samples$BUGSoutput$sims.list$theta)
qc$plot_scatter(x = samples$BUGSoutput$sims.list$theta)
# qc$plot_dens_gif(x = samples$BUGSoutput$sims.list$theta)
```


# parameters recovery RB
```{r}
n_reps = 10

res <- NULL
for (i in 1:n_reps){
  print(paste("Curenntly running", i, "out of", n_reps, sep = " "))
    # sim
  theta = runif(1, 0, 1)
  rb_sim <- agents$RB(theta = theta, payoff = payoff)
  
    # fit
  samples <- jags.parallel(data = list(choice = rb_sim$choices, n_trials = length(rb_sim$choices)), 
                inits = NULL, 
                parameters.to.save = c("theta"), 
                model.file = "RB_jags.txt",
                n.chains = 4, n.iter = 5000, n.burnin = 1000 # warm-up
                )
  
  tmp <- tibble(theta_true = theta, map_theta = jh$jag_map(samples$BUGSoutput$sims.list$theta))
  
  res[[i]] <- tmp
}
results <- res %>% do.call(rbind, .)

plot(results$theta_true, results$map_theta)
qc$plot_actual_predicted(actual = results$theta_true, results$map_theta)
```


```{r}
payoff <- cbind(bandit_payoff(ntrials = 100, choice = "a", a_prop = .3, a_reward = 2), 
                bandit_payoff(ntrials = 100, choice = "b", b_prop = .8, b_reward = 1.5))

```

```{r jags models rw}
model <- "model {
  
    # infer starting parameters
  Q[1,1] ~ dt(0, .16, 1)T(0,) # half cauchy
  Q[1,2] ~ dt(0, .16, 1)T(0,)
  
    # infer internal states
  alpha  ~ dunif(0,1)
  beta ~ dt(0, .16, 1)T(0,)
  
  for (t in 2:n_trials){
    for (k in 1:2){
      # update utility Q for chosen option with reward on last trials
      Q_update[t, k] <- Q[t-1, k] + alpha*(r[t-1] - Q[t-1, k])
      Q[t, k] <- ifelse(k==choice[t-1], Q_update[t, k], Q[t-1, k])
      exp_p[t, k] <- exp(beta*Q[t,k])
    }
    for (k in 1:2){
      p[t,k] <- exp_p[t,k]/sum(exp_p[t,])
    }
    
      # make choice
    choice[t] ~ dcat(p[t,1:2])
  }
}"


writeLines(model, "rw_jags.txt")
```

```{r rescorla wagner sim}
  # sim
res <- agents$rw(payoff, alpha = 0.3, beta = 2)

samples <- jags.parallel(data = list(choice = res$choice, n_trials = length(res$choice), r = res$reward), 
                inits = NULL, 
                parameters.to.save = c("alpha", "beta"), 
                model.file = "rw_jags.txt",
                n.chains = 4, n.iter = 5000, n.burnin = 1000 # warm-up
                )

qc$plot_agent_rw(alpha_sample = samples$BUGSoutput$sims.list$alpha,
                 true_alpha = 0.3, 
                 beta_sample = samples$BUGSoutput$sims.list$beta,
                 true_beta = 2, 
                 choice = res$choice-1, 
                 p_1 = res$p[,2], 
                 reward = res$reward,
                 Q1 = res$Q[,1], 
                 Q2 = res$Q[,2])
```



```{r sim}
#'@title Bandit payoff
#'@description
#'Generate a payoff for an n bandit task, where n is the number of bandits
#'n is derived from the length of probs and reward
#'
#'@param props the probability of reward, can be given as a list should be the same length as reward
#'@param reward the reward for a given prob
#'@param loss the expected loss if NULL (default) assumed to be 0
#'
#'@author
#'K. Enevoldsen
#'
#'@return 
#'a matrix of size n_trials x n
#'
#'@references
#'
#'
#'@export
bandit_payoff <- function(n_trials = 100, probs = c(0.3, 0.8), reward = c(1, 1.5), loss = NULL){
  if (length(probs) != length(reward)){
    stop("length(probs) != length(reward), which the function assumes, make sure it is the case")
  } 
  if (is.null(loss)){
    loss <- rep(0, length(probs))
  } else if (length(loss) != length(probs)){
    stop("length(probs) != length(loss), which the function assumes, make sure it is the case")
  }
  
  res =  array(0, c(n_trials, length(probs)))
  for (i in 1:length(probs)){
    res[,i] <- rbinom(n_trials, 1, probs[i])*reward[i]
    res[,i][res[,i] == 0] = loss [[i]]
  }
  return(res)
}
bandit_payoff(100, probs = c(.3, .8), reward = c(1, 1.5))


#'@title simulate fit and compare your jags models and agents
#'@description
#'
#'
#'@param todo
#'
#'@author
#'K. Enevoldsen
#'
#'@return 
#'a dataframe
#'
#'@references
#'
#'
#'@export
sim_fit_compare <- function(payoff_gen_fun, agent_funs, params_to_save, model_filepath, n_sim = 1){
  if (n_sim > 1){
    for (i in 1:n_sim){
      print(paste("Currenty running simulation: ", i, " out of ", n_sim, sep = ""))
      tmp = sim_fit_compare(payoff_gen_fun, agent_funs, params_to_save, model_filepath, n_sim = 1)
      tmp$n_sim = i
      if (i == 1){
        res <- tmp
      } else {
        res <- rbind(res, tmp)
      }
    }
    return(res)
  }
  res = array(0, c(length(agent_funs), length(agent_funs)))
  res = as.data.frame(res)
  colnames(res) = names(agent_funs)
  res$model_fitted_to_data = rep("NA", nrow(res))
  
  payoff <- eval(parse(text = payoff_gen_fun))
  
  for (a in 1:length(agent_funs)){
    nam = names(agent_funs[a])
    print(paste("Currently similating and fitting to agent: ", nam, sep = ""))
    sim_dat <- eval(parse(text = agent_funs[[a]])) # simulate data
  
    # fit each model to data
    for (i in 1:length(params_to_save)){
      samples <- jags.parallel(data = list(choice = sim_dat$choice, n_trials = length(sim_dat$choice), r = sim_dat$reward), 
                    inits = NULL, 
                    parameters.to.save = params_to_save[[i]], 
                    model.file = model_filepath[[i]],
                    n.chains = 4, n.iter = 5000, n.burnin = 1000)
      res[i, nam] <-  samples$BUGSoutput$DIC
      res$model_fitted_to_data[i] <- names(params_to_save[i])
    }
  }
  return(res)
}

payoff_gen_fun = "bandit_payoff(100, probs = c(.3, .8), reward = c(1, 1.5))"
agent_funs = list(rw = "agents$rw(payoff, alpha = runif(1, 0,1), beta = runif(1, 0.5, 3))",
               rb = "agents$RB(payoff, theta = runif(1, 0,1))")
params_to_save = list(rw = c("alpha", "beta"),
                      rb = c("theta"))
model_filepath = list(rw = "rw_jags.txt",
                      rb = "RB_jags.txt")
results <- sim_fit_compare(payoff_gen_fun, agent_funs, params_to_save, model_filepath, n_sim = 100)

results %>% write_csv(., "rw_rb_n_sim_100")
```






